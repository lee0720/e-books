---
html:
  embed_local_images: false
  embed_svg: true
  offline: false
  toc: true
print_background: false
export_on_save:
  html: true
  puppeteer: true
---
### 数据库设计思想

数据库设计是很典型的案例，通常我们用它作为学习的样本，在设计中有些模式也会被鉴戒或者被引入其他的领域。首先数据库本身是非常庞大而复杂的概念，跟数据库相关的东西很多，如何在不同类型数据库之间抽象出很好的设计是很难的事情。标准库提供的是
API 设计而不是框架设计，跟数据库相关的框架可能是利用辅助类型快速创建 SQL 语句，读写操作类似 ORM
框架，但是标准库并不会提供这些。任何一种框架都会有很多不同类型组成，它的关系相对来说非常复杂，内部也会涉及大量的动态特征，这些都不是标准库应该承担的责任。

从设计角度来说，实际上是一个倒金字塔结构。底层是汇编语句，它不复杂只是单向操作或者二元操作。在这之上是系统调用，系统调用指定命令、编号和参数，它没有上下文状态。在这之上提供库级别的
Api，它的调用格式与系统调用不一样，API 设计可能是系统调用的包装，也有可能是用户空间的函数类型，理论上也不维持状态，它提供复合类型的调用。

很多编码规范建议标准库不要提供全局设置，Go 在这点上做的并不好，标准库提供了很多默认全局变量，很多严谨的语言不这么做，Go 这种习惯显然是来源于对 C
语言的继承，C 语言因为早期的编程理念有这样的痕迹，新的语言其实很少这样做。在这之上提供框架级别的 API，维持状态。对于标准库来说它提供的是库级别的
API，它不提供框架级别的 API，框架级别的 API 复杂度非常的高，耦合性非常强。所以我们会选择第三方的框架去使用。在往上是 App 层面。

在哪个层面决定设计复杂度和完成度，对于标准库来说设计数据库相关的操作是很麻烦的事，RDBMS 数据库很多种，比如
SQLite、MySQL、PostgreSQL，怎么样设计支持这么多类型？不仅仅支持 SQL99
标准，很多数据库有不同的特性，这些特性决定了它的性能和调用方式，甚至 MySQL、PostgreSQL
通讯协议都不一样，怎么样在这些复杂的特性当中抽象出通用的设计？

多数情况下也是有章可循的，标准库只提供基本功能，只照顾到 80% 通用功能，20%
最关键的性能相关优化不管，因为标准库不可能随着数据库做同步升级，标准库设计立场是只照顾通用功能，如果需要使用定制功能使用厂家的专用版本。这个设计很常见，类似显卡驱动有两种，第一种是
Windows 自带的，第二种是厂家提供的驱动。

从设计上来说，首先提供一套 API 给用户使用，这套 API 跟某个具体数据库无关，抽象一些功能，比如管理会话 DB、连接
Connection，它是用来管理连接并不创建连接，真实连接由具体数据库管理，它是把真实的数据库连接做包装，因为用户需要访问相应的细节。另外包含执行器
Exec/Query。

然后提供一套驱动程序的 API 接口，由不同的厂家或者第三方开发专门的驱动实现，比如基于 SQLite、MySQL、PostgreSQL
实现。用这种方式实现隔离普通用户和驱动，普通用户根本看不到具体哪一个驱动，只需要调用通用 API。类似实现有很多，Windows 不关心什么型号显卡，通过
DX 的绘图、渲染 API 进行交互，DX 通过驱动和具体的显卡打交道。写文件也是一样，有 NTFS、Fax32、Fax64 不同格式文件系统，用户通过
WinAPI 读写。

这种设计在很多场合都能看到，就是分离抽象和实现。用户层面的 API 完成不了实际操作，它只是负责暴露 API
给用户，接收用户的参数，最后转交给驱动程序完成真实操作。设计中有很多种不同实现的时候，我们会抽象出通用的接口，把通用的接口暴露给用户，隐藏变化的部分，公开不变部分，让用户只依赖于抽象接口有助于隐藏变化。

### DB

用户通过通用 API 访问，怎么知道选择哪个驱动呢？或者驱动通过什么方式选择呢？我们先研究驱动程序怎么注入。

    
    
    package main
    
    import (
        "database/sql"
        "fmt"
        "log"
    
        _ "github.com/mattn/go-sqlite3"
    )
    
    func main() {
        db, err := sql.Open("sqlite3", ":memory:")
        if err != nil {
            log.Fatalln(err)
        }
    
        defer db.Close()
    
        if err := db.Ping(); err != nil {
            log.Fatalln(err)
        }
    
        fmt.Printf("%+v\n", db.Stats())
    }
    

我们用的是标准库 database/sql 提供的 API，`_ "github.com/mattn/go-sqlite3"` 语句中
github.com/mattn/go-sqlite3
是标准的驱动程序，导入执行初始化操作完成驱动程序注入的过程。它只负责注入，接下来不会和这个库打交道，除非使用它的定制功能。

接下来用标准的操作，比如打开和关闭数据库，Ping 操作是向数据库发送简单的命令检查数据库连接能不能建立。

当我们用 `import _` 语句导入的时候会激活 init() 初始化操作，每个包都有一个或者多个初始化函数，初始化函数在导入时执行。函数内部执行
`sql.Register("sqlite3", &SQLiteDriver{})` 调用，创建具体的 SQLiteDriver 驱动程序，名字为
sqlite3 完成注册操作。

Register 操作内部有两个全局变量，一个是用来做同步的，一个是用来记录所有注册好的驱动程序，因为在单个应用程序中可能使用不同类型的数据库，比如用
SQLite 做本地缓存，用 MySQL 做大数据处理。由于字典是全局变量，必须做锁操作，同时检查同一个名称多次注册，然后把驱动程序对象保存到字典里。

换句话说，用户执行导入操作的时候，会激活具体某一个驱动的初始化函数，这个初始化函数会把自己注册到全局变量字典中。每种驱动都有唯一的键，这样就完成了驱动程序的注入操作，这有点像反向注入操作。

注册完以后，接下来所有的操作怎么移交给具体的驱动程序？当我们进行 Open
操作时候，必须指定名字，这个名字就是注册时提供的名字，还包含数据库连接，每种数据库连接字符串不一样。它检查注入的驱动程序列表，找到具体的驱动程序，创建 DB
对象。到目前为止我们只是为数据库操作提供了一些准备，并不需要提供一些复杂的处理，因为接下来所有操作跟连接有关系，必须通过连接传递相应的协议。对于抽象层面的
API 只负责管理状态和请求转发，并不负责提供具体的操作，具体的操作由驱动完成。

我们可以看到跟某个具体数据库有关的内容包含注入具体的驱动，接下来选择所使用的驱动名字。

通常情况下，DB 在应用开发中生命周期会非常的长，DB 本身不是数据库连接，它只是用于存储访问数据库的参数状态。

DB 这种设计思路是分离抽象和实现，这个抽象不是简简单单的接口，而是有具体操作的，和通常的 Interface
不是一回事，它本身需要维持一些状态，这个状态是一旦创建以后的状态，它本身是一套对象集合。它不但管理状态还管理连接池，因为驱动程序才不关心连接池，请求打开连接，至于怎么管理连接和驱动程序无关，驱动程序只和数据库快速通讯，解析协议，分析相关命令，处理数据，但是确定不处理任何与状态有关系的内容，所以驱动程序从设计角度来说是无状态的，更像是系统调用。所以连接池实际上是由用户
API 管理的，因为连接池相关参数是要暴露给用户的。

### Connection

连接池的设计直接影响系统性能，连接池设置太大会导致后台数据库压力非常的大，设置太小会导致前面的访问性能很低。

连接池有几个基本的参数设置非常重要，MaxOpen
最大能打开多少，每种数据库在安装完之后针对当前这种环境通常会设置最大连接数量，超出这个数量可能会导致很多的问题，通常连接池也会这样设计，所以有个最大数量。但是最大数量不等于连接池里面对象数量，换句话说，连接池本身是设计上很别扭的东西。

设置大小为 10 个的连接池并不意味着里面有 10 个连接，连接池通常有两个参数，MaxOpen 最大能打开数量，MaxIdle
最大空闲数量，连接池内部真正保存的是最大空闲数量，比如最大能打开数量 100，最大空闲数量 2，也就是对这个应用来说可以创建 100 个连接，但是当这
100 个连接被放回连接池时候，连接池只负责维持 2
个连接其他都会释放掉。连接池不等于对象池。连接池的作用不是对象的重复使用，它更多时候是为了有用户等待的情况下提前准备好所需要的连接，所以对连接池这两个参数要分清楚什么意思。

实际上最大打开数量等于当前正在使用的数量加上空闲数量。每种参数有个默认值和阈值，超出阈值就得等待，问题在于等待是个麻烦的事情，比如最大打开数量
MaxOpen 是 10 个，全部用完 MaxIdle 等于
0，接下来还有请求，很显然超出了最大打开连接限制。会涉及一系列问题，请求需要排队，因为按照用户逻辑来说是有先后次序的，有的先请求不能到最后才拿到那可能会饿死，我们知道锁可能会被饿死的这种状态，不能用同步方式简单用锁的方式来处理等待队列，可能新的比排在前面的先拿到锁，前面优先排的因为某种原因拿不到锁饿死，所以不能用普通锁机制维持。也就是说等待队列怎么管理是第一个麻烦。

第二个麻烦是当打开的连接用完以后放回连接池的时候会出现连接有可能坏掉，因为执行非法操作被数据库端主动关闭掉，代码有问题主动关闭掉。关闭掉的连接不能放进连接池。

第三个麻烦有些时候因为意外可能连接不了需要重试，重试几次合理呢？还有种麻烦我们要尽可能快的为等待队列提供已经连接好的数据库连接，不能每次拿到一个资格然后主动调用连接操作，这种效率会很低。通常做法我们为海量排队提前准备好连接，问题怎么提前？谁来提前？显然提前准备必然用单独并发单元去做。连接池的设计比你想象的要复杂，连接池本身涉及到一系列的问题，标准库是怎么解决这个问题的。

首先看怎么获取连接的。Ping 默认情况下会拿到一个连接，然后测试这个连接是否有效，这是很简单的研究入口。`dc, err :=
db.conn(cachedOrNewConn)`
只是尝试获取一个有效连接，因为真正创建连接操作是由驱动程序完成的，驱动程序怎么完成的我们不关心。它给的策略是 cachedOrNewConn
要么从连接池里找个已经缓存好的连接要么创建一个新的。连接池在初始化状态下里面什么都没有，所以给个策略，db.conn()
是获得一个连接，使用缓存过的或者新建的，那么它拿到连接以后这个连接必然是活的是可用的连接。然后把连接放回到连接池里面去。

#### **conn 函数**

conn
函数有连接策略，这个策略包括缓存或者新建。有些情况下执行系统命令不和用户竞争使用策略就是新建，这样可以绕开连接池。用策略的方式形成分支。如果有缓存优先命中缓存，如果没有就新建。

**第一步尝试从连接池找到空闲连接** ，根据策略判断：

conn
函数检查连接策略，同时检查当前有没有空闲连接，如果有空闲连接拿到空闲连接，连接是切片，优先返回第一个连接然后把剩余的调整一下。用一个切片管理多个空闲的连接，把
0
号取走了以后必然把后面的内容往左移。因为连接池大小是固定的，类似垃圾回收一样取走一个，右边往左移，这样右边就空出来了，不再使用返回的连接就可以往后追加，这是很简单的操作。

**第二步检查是否超出连接池的上限** ：

如果没有的话，检查有没有达到连接池设置的上限，当前已经打开的连接是否超出这个上限，如果超出上限的情况下把自己放入请求队列里面去，如果拿到连接就返回否则一直阻塞在那，用
channel 实现阻塞。

如果既没有缓存又没有超出上限，直接通过 driver 新建一个连接就可以了。

整个我们发觉从数据库里面取连接，它的策略其实很简单，无非是先命中什么，关键问题在于如何等待。

#### **putConn 函数**

把连接放回来，现在用完了放回连接池怎么样操作呢？要检查这个连接是不是已经坏掉了，坏掉的情况下它会调用 Close 关闭相应的资源，这个 Close
不是关闭连接，它是要清理用户 API 层面上的资源。如果坏掉它只是清理资源并没有把坏掉的连接放到连接池，因为没有意义这个连接用不了。

如果没有坏掉就把它放回连接池里面，同时通过返回值判断添加是否成功了，如果没有添加成功就放弃。

为什么连接没有坏掉会添加失败呢？我们可以打开 100 个连接，但是连接池只保留 2 个空闲，里面已经有 2 个空闲肯定不放回直接释放。连接池只保留 2
个，如果 2 个都拿走了，那么接下来拿第三个的时候新建，第三个放回来时候就得检查连接池里是否已经有 2 个了，已经有 2
个情况下第三个就不要放回来直接释放。所以就算正常的连接也有可能添加失败。连接池空闲数量和最大打开数量概念不是一回事。

#### **putConnDBLocked 函数**

往回放的时候会检查最大空闲数量，同时还要检查请求队列，优先检查请求队列，比如连接往回放的时候，优先放回连接池里还是优先满足排队的？肯定是优先满足这些排队的。

比如第 3 个连接放回连接池的时候，连接池已经有两个了，首先检查有没有排队，如果有直接把连接给它。

连接池两个属性，最大打开和最大空闲，最大空闲才是连接池一直保持的数量，假如最大空闲是 2，最大打开是
100，当我们去拿连接的时候首先从最大空闲里面找，有的话直接返回，如果没有的话就新建，只要当前使用的不超出 100
就可以了。那么连接放回来时候就会涉及第一种连接池内部已经满了，就放不回来了，因为没有位子了，就 close 掉，在 close
之前除了往连接池放以外还有请求队列在等着，所以优先满足排队的，然后尝试往连接池放。所以往回放首先检查请求队列是否为空，不为空的话会优先把连接交给排队的。

大概了解了连接池的操作方式，剩下来的问题比较简单了，请求队列到底用什么样的机制工作的？

#### **DB 结构体**

DB 跟连接有关系的有几个字段，freeConn 真正的空闲连接就是最大空闲数，connRequests 是排队请求的，numOpen
当前已经打开的连接数量，涉及到 MaxOpen 阈值检查，openerCh
是事件通知，排队请求拿到的连接最好是已经创建成功的，要么是还回来好的连接，如果不是的话最好有人帮它们提前准备好，提前准备好从执行逻辑上来说就必然有并发单元在后台工作。

#### **connectionOpener 函数**

connectionOpener
就是很简单的循环，为等待队列服务的，它的工作原理非常简单，从通道接收通知，往通道丢什么数据不关心，只要有数据就工作一次，它是一个同步通道，有人通知就尝试创建新的连接，openerCh
通道是为 connectionOpener 提供创建连接通知的，因为 connectionOpener
必须知道什么时候去创建，如果使用循环资源消耗就太大了，所以实际上每通知一次就创建一次。

#### **openNewConnection 函数**

openNewConnection 创建操作很简单。

接下来看第一个问题谁来创建通知？第二个问题创建好的连接怎么样交给请求队列？

#### **maybeOpenNewConnections 函数**

在 putConn 把连接放回去，当连接坏了情况下会执行 maybeOpenNewConnections
调用，它会检查当前是否有人在排队，检查排队是否超出最大上限，比如最大打开数是 100，可能有 200 个在排队，那么不能超出 100 上限，只能发出 100
次事件通知，让前 100 个拿到通知先工作，后 100 个接着排队等，排队数量可能超出 MaxOpen
数量。所以要修正一下这一次大概要通知多少人，拿到通知数量之后执行 for 循环，向 openerCh
通道里面发送数据，丢一次数据接收一次创建通知，一共丢多少次通知多少次，这样一来 connectionOpener 根据连接通知就创建新的连接信号。

谁来发出信号的？是往连接池里放连接时候发现连接坏掉了或者连接被关闭掉了，也就意味着原来可以正常工作的没有超出上限的，但是连接坏掉了，那就创建新的连接补上。每打开一个连接就要修改计数器，结果计数器没改连接坏掉了，那把坏掉连接丢掉创建新的连接来补回去，这样来保证
MaxOpen 和 MaxIdle 数字平衡。所以给 openerCh
创建事件通知，因为某种原因连接关闭，需要使用新的连接替换，这样维持后台的连接一直处于有效的状态。提前准备这些连接。有两种方式发送这种通知，第一个是连接坏掉了，第二种连接主动关闭掉了，主动关闭会间接的调用
removeDepLocked 方法。

我们现在知道 connectionOpener 在后台专门提前创建连接的，有专门通知通道 openerCh，连接坏掉和主动关闭创建通知。

**接下来问题是，怎么样把连接交给请求队列？**

有两种方式，第一种由 connectionOpener 来交，第二种把连接放回连接池的时候。

conn 函数里所有排队的会创建一个通道，然后把通道放到全局列表里去。假设 A
处于排队状态，它会给自己创建一个通道，通道可以做两件事，第一可以用通道接收连接数据，不关心连接从哪来的只要把连接放到管道里就行了，第二接收事件，当接收数据的时候就代表事件被激活了，如果接收不到事件的时候实际上处于阻塞状态。那么
A
创建一个通道用来接收连接对象，创建完然后把自己加到排队列表里去，然后自己尝试在自己带的管道里面接收数据，接收到了就解除阻塞了，接收不到表示阻塞状态，处于阻塞状态时候表示业务逻辑停在这了，不需要任何的回调，因为没有拿到连接就阻塞在这。

这样一来整个设计就会变得很干净，我给自己提供事件通道，这个事件通道接收数据，然后把自己放到排队列表里去，接下来我就看着事件通道，有数据的话就解除阻塞没数据就等着。这样一来逻辑会很干净，看上去没有和其他东西有交互操作，交互操作的话就是跟管道操作，不需要把逻辑打乱，逻辑还是同步方式，不会看到有异步调用也不会看到有
callback 回调。

创建一个管道，把管道加到排队列表中，接下来等待，等到的结果肯定是要么有连接要么管道坏掉，所以用几行语句就完成了排队和获取连接的操作。没有任何异步调用没有任何回调操作，它的逻辑非常干净。因为接下来管道被谁操作跟它没关系，我只提供数据接收的途径，至于谁把数据交给我不是我要管的事。对于
A 来说只提供接收的途径，至于谁把连接放进来不关心，A 的目的只需要拿到数据库连接，它没有职责去管理连接操作，它的职责只有获取连接。

Go 有个好处 goroutine 机制加上 channel 机制把同步代码写的非常干净，不需要用异步调用也不需要用回调。

我们现在知道排队时候处于什么样的状态，接下来的问题谁去处理放入操作，谁把连接最后放进来，放进来就意味着激活了 A，把阻塞状态变成了非阻塞状态。

#### **putConn 函数**

putConnDBLocked
会检查请求队列是否为空，如果不为空，会从请求列表拿出第一个，然后把后面的往左移，把后面位置空出来。拿出来之后就能访问它对应的管道，接下来直接把连接丢到管道中，也就意味着阻塞激活了。

所以第一种往回放连接的操作很简单，把排队列表里 0 号拿出来，接下来把后面向左移。直接把放回的连接丢到管道里就可以了，也就意味着阻塞激活了。

第二种方式是连接有可能是坏的。connectionOpener 在后台怎么去完成的，它调用 openNewConnection
方法，它会创建新的连接，接下来 putConnDBLocked 同样往回放，这个操作和上面第一种是一回事。

那么我们现在知道了整个队列是怎么样唤醒的，怎么样激活的。关键是利用管道机制分离职责，只负责获取连接，至于连接谁创建谁放进来和我们没关系。

使用管道完成两件事，第一个用来接收数据，第二个阻塞事件，这样我们代码变得干净简单，这是一种很常见的技巧。

#### **小结**

设计连接池的时候关键在于考虑哪些，有哪些参数限制你做这件事，是否能考虑完整，另外我们为什么用排队机制，这种排队机制为什么不能用锁机制来代替呢？因为锁机制可能会导致其中某个饿死，先进来用锁并不能保证可以优先拿到连接。锁机制不等于排队，锁只能表示在同一时间之内只有
1
个人或者指定几个人在做这个事，但是锁不能保证先后顺序。所以设计排队机制要保证公平性，谁先进来的谁要先完成，因为数据库操作谁先触发理论上存在前后顺序和优先级的问题。

还有最大打开数量和最大空闲数量是两回事，因为对象池只有一个数据，一共存多少个，但是连接池不一样，在设计上比对象池复杂的多。

### 坏连接是谁的职责

从连接池里拿到的连接有可能是坏的，因为放到连接池里面连接可能很长时间没有响应过了那么有可能会被服务器给关掉，数据库服务器有可能中断掉这个连接，因为长时间没有活跃了，不是所有的数据库都会维持一个心跳，数据库的心跳和
TCP 的心跳并不是一回事，也未必用的是 TCP 连接，这些东西很复杂，不同的数据库有不同的做法。

如果我们拿标准库 API
去写一个数据库操作，我们需要管理数据库连接是否坏的么？我们需不需要用结构化异常保护这种东西？类似这样的问题在日常开发中类似场景有很多，比如打开一个文件，我们在打开时候可能判断这个文件是否存在，当我们打开的是日志文件，长时间去写，有可能遇到这样一个问题，文件莫名其妙损坏了删除了，甚至磁盘满了，类似这样的问题我们需不需要在我们逻辑中保护呢？就算保护我们能考虑周全把所有的问题都保护起来么？还是说我们捕获一个错误然后去重试，那么磁盘满了捕获错误重试也没有任何意义，因为写不了数据了，这时候有没有后备策略？诸如此类的问题会非常的多，比如写
Redis，Redis 挂了，需不需要保护？这些问题到底应该谁来负责？

数据库连接出错这种事到底谁来负责？任何一件事分清楚到底属于谁的责任。职责划分不清楚的情况下代码肯定是有些坏味道。写 App
关注的是逻辑，可能要捕获一些异常，但是你有没有责任维护异常修复这件事，异常修复到底谁来负责？异常修复并不属于逻辑，因为我们写逻辑设定的场景是数据库、网络通信是可用的，所以去做这些事情的时候可能要去做，但是未必是你要做，也可能
API
本身已经保护好了，也有可能需要开发类似的插件，比如中间件专门开发维护某些意外的场景，但是这个东西不属于逻辑，去维持一个环境属于底层支撑系统，它不属于逻辑，它的层级要比逻辑更低。

当我们拿到一个很陌生的第三方 API
库或者数据库框架的时候，我们怎么知道它是否具备类似的能力呢？这个需要我们阅读这方面源码，因为我们要知道对方能做什么不能做什么，对方什么东西做的很好，什么东西没做好。只有这样我们才能保证在上线之前我们要补充什么。

当你开发逻辑的时候，最好先了解下这些东西，然后先不要考虑那些坏的事情，先把逻辑完成了，然后所有单元测试都通过了，在这些通过以后再去考虑出现这样的问题应该怎么办，因为这些问题往往不是代码本身就解决了的，比如说磁盘满了，就算拦截这个错误也解决不了，你可能需要考虑整体架构，当出现这样的问题之后是否需要查询路由表或者是分片表找到下一台服务器去写，也就是说拿到错误未必能解决。那么当你写逻辑的时候不要考虑这些，这些东西职责不属于你。当你从逻辑上抽身的时候，这时候你所承担的责任是解决这些问题。虽然是同一个人，但是在不同时间上你的责任是不一样的。

一个坏的连接谁在管？连接池是否有责任维护这个事，是否有责任保证拿到的连接是好的，未必。因为对方到底做不做对方跟你之间没有明确的约定。所以你需要阅读对方的文档和源码来确认这件事，如果对方已经做的我暂时不做，如果对方没有做那么我们考虑去做。

那么我们从数据库操作最简单的操作呢是相查询，对吧？说出这套的无非是增删改、查嘛查询是最简单的一种操作。啊为此呢我们用小 case 呢

### Query

数据库最简单的操作是查询。建立一个数据库，数据库的结构非常简单。

    
    
    CREATE TABLE IF NOT EXISTS user(
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT NOT NULL,
        age INT DEFAULT 0
    );
    
    
    
    $ sqlite3 test.db
    sqlite> .schema user
    sqlite> select * from user;
    

代码首先打开这个数据库，最后记得数据库关闭，然后执行查询操作，它可能会返回错误，因为可能某种原因拿不到数据，这儿仅用日志方式保存。这时候查询返回是多条记录，我们需要通过循环做迭代。这地方有个别扭地方，因为
Go 并没有为 for 循环提供一种类似迭代器一种设计，除了几种内置类型支持 for range
迭代，其他的不支持，所以这地方用两个函数调用来实现的。rows.Next() 确保能返回下条记录，返回以后会保存到内部 buffer
里。rows.Scan() 把 buffer 里面的数据转换成目标类型最后保存到临时变量里面去。Scan()
通过指针可以改变这两个变量的值。最后要关闭这个循环。

    
    
    func main() {
        //打开数据库
        db, err := sql.Open("sqlite3", "./test.db")
        if err != nil {
            log.Fatalln(err)
        }
    
        //关闭数据库
        defer db.Close()
    
        //执行查询操作，可能返回一个错误，可能会因为某种原因拿不到
        rows, err := db.Query("SELECT name,age FROM user WHERE id > ?", 1)
        //错误怎么处理是另外一回事，这里仅仅用日志方式保存下来
        if err != nil {
            log.Fatalln(err)
        }
    
        //查询可能返回多条记录，用 for 循环做下迭代
        for rows.Next() {
            var (
                name string
                age int
            )
            //输入指针的话可以改变变量的值
            if err := rows.Scan(&name, &age); err != nil {
                log.Fatalln(err)
            }
    
            fmt.Println(name, age)
        }
    
        //最后关闭 for 循环
        rows.Close()
    
        if err := rows.Err(); err != nil {
            log.Fatalln(err)
        }
    }
    

整体上来说用 API 去操作数据库的话没什么复杂的东西，那么我们主要看下内部做了哪些事情。

首先它提供了坏连接重试次数全局设置，就是当我们查询时候，如果连接是坏的它会进行重试，首先进行查询的时候会尝试先拿到连接，它拿连接的策略优先是优先从连接池里面拿，如果连接池拿不到就新建，执行时候它会检查返回值，如果返回值表示连接是坏的那么它就会重试，它设置的全局次数是
2。

为什么只重试 2
次呢？经常阅读第三方源码时候我们发现这样的魔法数字非常的多，这个值到底是怎么出现的，其实呢这个魔法数字很好理解，当你从连接池里尝试去取连接，如果连续两次都拿到坏连接情况下，这时候有两种可能，连接池里面连接都是坏的或者大部分是坏的，这个时候没有必要再重试第三次，从概率上来说已经够倒霉的了，重试第三次情况下未必有好的结果，2
实际上是最小的幸运数字，当然这和连接池的设计有关系。

如果两次都不能解决坏连接问题，采用新的策略是新建一个。新建一个保证的是要么可用的连接要么网络或者服务器挂断，挂断的情况下是修复不了的，所以在新建策略下没有必要再去重试了。

这段代码不复杂，涉及到策略设计的时候，很多人代码写的非常僵化，没有思想在里面，当出错考虑重试的时候肯定不停的去连接池里拿直到拿空了为止，可能不会去想最多拿
2 个，这体现的是灵活的态度，这个态度是因为有想法。

其实错误重试 API
已经帮助我们解决了，但是我们依然要捕获这种不可修复的错误。所以我们只是简单的把错误保存到日志里面去，因为用户也解决不了这个错误，保存在这以后进程挂掉，系统管理员通过信息大概能知道服务器出现什么问题了，当他修复的时候可以重启这个进程或者是提供后备方案，就是提供一个中间件来处理，后台有多台数据库有灾备容错体系，出现某些错误时候可以进行重试，这件事为什么最后做呢？

因为这个事本质上和逻辑没有关系，首先保证逻辑是对的最后再解决这个问题，写逻辑时候解决这种问题没有任何意义，因为解决不掉，因为 App
开发人员连后端的系统架构都不知道。但是必须把这个错误保存起来，所以在这个时候不用重试数据库重试操作，因为系统 API
已经做了这件事，同时它的重试策略比用户写的更合理，因为我们在外面没有办法控制连接池的，我们处理不了获取连接策略。

在查询操作 queryConn 时候，实际上给了 releaseConn 参数，实际用途就是将连接放回池。Query
返回一个记录集，它并没有关闭连接也没有释放连接。

当进行 for
循环取多条数据的时候，这时候连接必然是打开状态的，因为数据库不可能一次性把查询的结果全部推到客户端，它实际上是按照单条迭代的，就是一行一行去拿，直到 for
循环结束，这个连接必须保持打开状态，实际上一直持有这个连接，最后 rows.Close()
确保连接放回到连接池里面去或者是把这个连接释放掉。换句话说，连接在这才会释放，这就会导致在执行 for
循环时候连接一直处于打开状态，这需要注意的是有时候我们拿到数据执行某些逻辑非常复杂，这些逻辑需要花费很长时间，这种执行方式会导致连接长时间被占用。

数据库连接是非常宝贵的资源，这种资源竞争效应非常大，所以我们逻辑执行时间非常长的情况下其实并不适合在这地方直接处理逻辑，这样直接处理的方式可能带来很多的麻烦，因为连接长时间被打开。如果
QPS
压力非常大的情况下会导致并发效率非常低，因为连接被打开但是大部分时间是执行逻辑操作，这个连接白白的被占用。这时候可以把查询出来的数据丢到队列里面去，后端有并发任务异步处理取到的数据，这个并发任务用来执行逻辑，对于查询语句时间就会很短，长时间占用的逻辑由并发任务处理，这样可以保证循环快速结束，确保尽可能快的把连接归还。除了保证逻辑是对的还要保证关键性资源尽可能快的释放。

queryConn 操作，执行查询操作肯定交给 driver 执行，究竟是
SQLite、MySQL、PostgreSQL，因为本身执行不了查询器操作的，只不过把这些通过一定途径交给 driver
处理。它是用两种方式执行查询，首先普通查询方式，后面用 Prepare 方式执行。

普通查询方式从底层驱动器找到查询器，然后把相应的参数交给它，如果查询出错的情况下释放连接。因为这种出错未必是连接导致的，可能是 SQL
语句有错误或者其他原因，在这一行出错后边的执行没有任何意义，通常会终止这个程序的执行。如果没有出错的情况下会尝试拿到记录集，这时候会再次判断错误，这里有专门的错误标识是
driver.ErrSkip，可以通过这个告诉底层驱动程序，就是拿 10 条记录拿 3 条就够了，通过特殊标记告诉驱动后面记录不要了也可以释放掉了。

**正常方式执行就够了，为什么还会用 Prepare 方式执行？**

我们知道数据库执行有两种方式，一种是普通方式直接用 SQL 语句加上参数去执行，另外一种方式当一条 SQL 语句被重复执行的时候，可以预编译 SQL
语句，先把 SQL 语句传给数据库让数据库先编译好，接下来只需要传参数，这时候它的执行速度会快很多，但是这是指的情况下是执行多次，如果只执行一次的情况下
Prepare 方式会很慢，因为预编译方式有多次的网络交互，首先 SQL 语句通过网络传给数据库，数据库编译编译完了返回 PrepareId，接下来用 Id
加上参数去执行，这样会多次的网络交互。但是只执行一次最好的方式是直接把 SQL 语句传给数据库返回结果，这样只有一次网络交互。所以 Prepare
方式快的前提是要多次执行，单次执行的话它花费的成本会高很多。

所以 queryConn 首先用普通的方式去执行，就是说它尝试用 SQL 方式执行而不是预编译方式，但是这设置的是理想状态，实际上底层 driver
到底用什么方式执行这地方不知道，因为很多数据库是这样的，就算单次执行但是里面带参数占位符情况下它会自动转换成编译方式执行，有些时候直接执行一条语句的情况下建议直接不使用参数占位符，这样的执行效率比参数占位符快，因为参数占位符很多数据库只要发现
SQL 有参数占位符自动用预编译方式执行，可能效率会很低。

用参数的方式可以避免数据库注入，如果不使用参数方式就有责任检查 SQL 是合法的安全的。类似这种 SQL 检查的库有很多，很多第三方库可以检查 SQL
数据库语句安全。这种好处单次执行效率比预编译快很多，很多时候在数据库优化时候如果是单次执行建议去掉参数。

所以对于 queryConn 来说，首先尝试用普通的方式去执行，如果普通方式执行有问题时候会尝试使用预编译方式执行。

我们看到 API
层面设计的时候，它的取舍它对性能的考虑，其实它关注的焦点不是代码好不好看，站在系统程序员角度思考的方式是不一样的，这段代码我们看到作者对于性能的考虑，它不知道底层
driver 是否支持占位符，单次优化方式，如果能支持就可以执行，如果不能支持，就用预编译方式执行。写 API
考虑的焦点在于尽可能快的完成要求。这是对于一件事不同的思考方式，起码它把这件事揽到自己身上，让写逻辑的人不用关心这件事情。

但是作为高端程序员起码要知道 Query
在底层到底怎么样执行的，如果我们在调优的时候会发觉预编译方式数据库一端是要维持相应的资源的，单次执行操作的话执行完了数据库不需要维护这种状态的，预编译方式需要维护，预编译必须维持着，下次传参数多次执行的时候才能找到这样的状态，所以预编译方式要对数据库压力大很多，它虽然执行速度会快，但对于数据库来说要消耗更多的资源。优化数据库查询操作起码知道问题出在哪里。

最后当查询完，调用记录集 Close() 的时候，它会尝试释放连接。所以当 Close() 连接才会归还，当然我们还要处理下错误信息
rows.Err()，因为调用 rows.Next() 时候可能就直接出错了。

对于数据库操作过程，比如哪一步处理错误可能有一定的规范有相应的模板，但是我们一定要考虑清楚 db.Query()
错误可能会出现什么问题。rows.Err()
出错是什么问题，同时为什么要必须关闭，是因为在这个时候这个链接才会真正释放掉。这一切我们必须要了解从始至终相关资源的生命周期。如果不了解生命周期，很多时候写的都有问题，很多人代码中对数据库的及时释放做得都不是非常好。

Next() 首先会拿到最新的一条记录，拿到以后存到 buffer 里面 lastcols 去，实际上是临时存储，因为 driver.Value
数据实际上没有被转化成我们目标类型。最后用 Scan() 转换成目标类型，所以性能并不是特别好。

还有一点，我们进行转换操作的时候，这个设计实际上是一个坏味道。错误信息是由 Scan()
返回的，但是并没有定义全局变量表明是哪种错误，这些变量都是新建的，所以我们没有办法判断错误到底是什么，程序中只能通过字符串来判断什么原因出问题了，如果字符串万一修改呢，所以当出错的时候只能记录下信息，具体什么原因搞不清楚，因为程序没有办法做语义解析，只能通过字符串匹配大概知道什么原因。但是问题在于这些代码是内部代码，它并没有对外公开错误全局变量，这会导致这几个错误信息可能在未来发生变化，API
作者没有责任保证字符串信息不会发生改变，如果保证可以写成全局常量。

写程序不能依赖错误信息，我们应该依赖错误类型和错误变量，因为错误类型和错误变量是明确的错误信息，错误的字符串提示信息不是写给代码看的而是写给人看的。

就是说对于 Scan()
返回的错误信息我们要不要处理？我们需不需要解析字符串信息匹配做出相应的处理呢？建议是永远不要用代码理解字符串信息做出相应的处理，因为这会导致几个问题，这样会依赖非公开的逻辑，处于危险状态，甚至都不会检测到已经出问题了。如果发现转换失败只可能出现下面理由，第一数据库架构发生变化，数据库架构导致字段类型，列的类型发生变化。第二是列是非空的，现在变为空了。第三插入数据可能出错了，导致编码解析出错。不管是什么错误导致的，其实不是程序造成的。只能纠错。把这段错误记录下来，然后让进程终止，或者是用其他的方式抛出一个不可修复性错误。

让一个进程挂掉是一种很合理的错误提示方式，实际上是向监控系统发出一个警报，如果你自己去尝试修复的话，系统监控的话是监测不到你这个修复操作的，向系统管理人员就发出警报的。

### QueryRow

查询单行就比较简单了，查询单行是对多行的包装，只不过它主动帮你调用关闭操作。

### Exec

除了查询操作以外，其他操作用 Exec 执行。把语句传到数据库执行直接返回状态。Exec
内部也会对拿到的连接做重试操作，执行完之后会主动把连接放回连接池，执行的时候也是先用普通方式执行，普通方式不行的情况下用编译方式执行。

对于数据库连接来说，我们其实要关注的是多行查询，单行查询和执行操作对数据库连接我们并不需要太多的干预。

### Prepare

预执行是什么意思呢？我们通常情况下跟数据库打交道用户端直接拿到一个连接，然后将 SQL
语句加上相应的参数丢过去去执行，执行完返回一个结果，通常情况下这个过程非常简单。但是我们知道数据库执行 SQL 语句其实要完成很多步骤，比如解析 SQL
语句，因为 SQL 语句本身并不是能执行的它只是想表达想干什么，毕竟不是可执行语言。如果频繁执行同一个 SQL
语句的话，这种执行方式代价比较大。预执行的过程相对来说比较复杂一点，首先在连接上把 SQL 语句传过去，这时候可能基于文本协议的，数据库解析 SQL
语句形成类似编译版本，相当于把 SQL 语句转换成函数调用，然后把相应函数 Id 发回给用户，这个 Id
实际上就是一个编号，在服务器端会保留相应的信息，接下来执行的时候可以用二进制用 Id
加上参数，这样就变成了函数调用，显然当你频繁使用这样命令的时候速度就会快很多，因为 SQL 做了预处理。

当然缺点也很明显，很显然下面这种操作过程比上面网络通讯环节更多，如果只执行一次的话下面的成本要高很多。多次执行有好处，它把第一次成本分摊到多次成本中去所以这样一来单次成本就会很低。显然对只执行一次操作预处理模式消耗的成本更高，因为它有多次的网络通讯过程。

**预处理内部怎么实现的？有哪些问题？**

简单方式是否使用预处理方式不是由标准库决定的而是底层驱动程序决定的，很多驱动程序发现有参数占位符自动使用预处理方式，这样的好处在于防止注入的问题，因为不用拼接
SQL 语句，SQL 语句第一次传过去后面传的是 Id 加参数这样就不可能形成注入逻辑，所以最后到底是用什么方式执行其实是由驱动程序决定的。

预处理分为客户端版本和服务器端版本，之前说的是服务器端版本，也有一种是客户端版本，通常情况下是有些 Fx 会实现，它在客户端把 SQL
解析好以后，本来加上参数是用预处理方式执行，它会在客户端完成拼接，同时进行类似注入检查这样的逻辑，但是这种东西很少见，因为客户端的检查是不安全的，在网络通讯过程中可能是被修改的。这些对于大多数开发人员不需要了解细节的，但是简单方式和预处理两种方式优缺点适合什么时候用需要搞清楚。另外需要了解数据库官方文档对预处理需要消耗什么。

    
    
    func main() {
        //打开数据库
        db, err := sql.Open("sqlite3", "./test.db")
        if err != nil {
            log.Fatalln(err)
        }
    
        defer db.Close()
    
        //语句传给数据库进行预处理返回包装对象
        stmt, err := db.Prepare("INSERT INTO user (name, age) VALUES (?, ?)")
        if err != nil {
            log.Fatalln(err)
        }
    
        for i := 0; i < 10; i++ {
            name := fmt.Sprintf("user%d", i+1)
            age := 10 + i
    
            //用包装对象去执行，需要传递参数
            res, err := stmt.Exec(name, age)
            if err != nil {
                log.Fatalln(err)
            }
    
            log.Println(res.RowsAffected())
        }
    
        stmt.Close()
    }
    

从代码来看好像很完美，好像多次执行的情况下没什么问题，实际上真的会这样么？

Prepare 把 SQL 语句传给服务器编译，编译完返回 `*Stmt`。首先尝试从连接池里面拿连接会进行重试，prepare 函数完成编译操作，
prepare 函数先拿到连接 dc，prepareLocked 把 SQL 语句通过底层 driver 传递给数据库，数据库编译完以后会返回 si
对象，也就是说以后进行操作需要两个变量 dc 和 si 配合。因为编译的 Id 和具体的连接有关系，Id
是建立在具体的某个连接上的不是全局的，就相当于如果每个连接相当于 Session 的话，这个 Id 相当于这个 Session 上特殊
Id。出错的情况下把连接放回去，接下来创建 Stmt 对象实际上把需要的东西打包。包括 DB 对象、SQL 语句、css 字段包含 dc 和 si，ID
和具体连接绑定。但是完全编译以后它并没有持有这个连接，它实际上把这个连接返还给了连接池。

Prepare 函数的连接和 stmt id，这两个实际上是绑定到一起的，问题在于在这连接显然是应该持有的，因为把连接丢掉以后 Id
就没有任何用处了。但是标准库把连接放回到连接池里面去了，这实际上就是很大的坑。我们首先要明白为什么放回去呢？放回去以后如果下面执行拿不到这个连接的话不就出错了，标准库实际上考虑的问题是数据库连接资源非常宝贵，在完成编译和执行的中间到底要消耗多长时间，因为很多做法是
Prepare 放到初始化函数里提前创建好，至于具体什么时候执行真的很难说，标准库不能假设这中间时间差非常短，所以对于非常宝贵连接资源来说必须先放回去。

标准库把连接放回去，这对我们有很大的问题，接下来真的去执行预处理语句的时候我们能不能拿到当时进行编译的时候连接，因为拿不到相同的连接 Id 是没用的，因为
Id 和具体的连接相绑定的，这地方存在很大的坑。怎么样拿到当时的连接呢，因为当时连接放回连接池有可能被别人使用了，也有可能连接坏掉了。

Exec 执行的时候会尝试多次获取连接，拿到连接操作是 connStmt
函数决定的，拿到连接以后接下来执行，执行完释放掉连接，这些不需要我们考虑，我们的关注焦点是怎么样拿到当时的连接。

connStmt
函数首先直接从连接池里尝试取连接，也就是说当执行的时候并没有通过某种途径找当时的连接，直接从连接池里拿一个连接，然后检查连接池取的连接是否是当时编译时候那个连接，这种几率很低尤其是在高并发情况下。

写数据库的压力测试，得到的结果可能非常不准确，它没有办法用来代替线上的模拟运行，正好命中最优状态，而上线的时候面临是最糟的状态。所以做数据库的压力测试的时候，尽可能的模仿乱序的高并发。

如果不是的话用新的连接重新编译一次，拿到新的
Id，所以从这点我们可以看出来标准库在这上面处理有优点也有缺点，优点在于标准库要照顾到很多不同的情况，而且要照顾不同的数据库类型，缺点在于这种做法太粗糙了，内部可能会带来很多性能问题。重新编译继续把
dc 和 si 保存到 Stmt 对象 css 里，保存到 css 里不是替换而是添加，也就是说 css 里面保存的是多个连接上的 Id
号，除非主动告诉数据库把连接上的 Id 号取消掉，否则数据库有责任维持这些 Id 号，也就意味着下次执行的时候命中任意一个都是可以的。

这会带来的问题是预处理执行可能会牵扯到多个数据库连接，虽然最后执行的时候只用了一条，问题在于服务器端数据库要维持这些资源有很大的开销。这就造成预处理在服务器端编译
Id
的数量大于前端的逻辑数量。这对服务器来说是很大的压力。起码从这点上可以看到预处理可能不是太美好。它有优点但是不是终极解决方案在某些时候会适得其反，因为这些资源对数据库来说是很宝贵的。尤其在高并发状态下，这种状态可能会导致服务器端的资源竞争的非常激烈。需要监控服务器的相关的资源，如果消耗的太严重的情况下，尝试退化成简单方式。

Close 主动释放相关资源。它会遍历 css，调用底层相关语句移除，这时候数据库才会把资源释放掉。所以 Close
是必须要执行的，否则需要等数据库主动回收。

我们现在看到预处理的确很好，但是好都是有限制的。常见的三种驱动程序怎么处理这种事的，我们看到如果参数为 0 就普通方式执行，参数大于 0
的情况下实际上是用预处理方式执行的。如果要优化性能的话就需要拼接成没有参数的。

### Transaction

在 Go 语言里，事务是怎么样的执行过程呢？执行一个事务不是很复杂，无非是启动一个事务，然后用事务去执行，但是有些技巧需要掌握。

    
    
    func main() {
        db, err := sql.Open("sqlite3", "./test.db")
        if err != nil {
            log.Fatalln(err)
        }
    
        defer db.Close()
    
        //首先启动一个事务，返回 tx 对象代表事务
        tx, err := db.Begin()
        if err != nil {
            log.Fatalln(err)
        }
    
        //把分散的代码块变成大的代码块
        //只要 err 出错就终止函数执行
        //在终止之前用 defer 判断错误不为空就执行回滚操作
        //这样只需要一个地方回滚操作
        //重构很常见技巧把分散的逻辑变成大的逻辑块，这样可以把分散逻辑重复的部分抽象出来变成统一处理
        func() (err error) {
            //出错回滚
            defer func() {
                if err != nil {
                    tx.Rollback()
                    log.Println(err)
                }
            }()
    
            //执行
            _, err = tx.Exec("INSERT INTO user (name, age) VALUES (?, ?)", "userx", 90)
            if err != nil {
                return
            }
    
            //查询
            var id int
            err = tx.QueryRow("SELECT id FROM user LIMIT 1").Scan(&id)
            if err != nil {
                return
            }
    
            println(id)
            return
        }()
    
        //事务提交
        if err := tx.Commit(); err != nil && err != sql.ErrTxDone {
            log.Fatalln(err)
        }
    }
    

首先启动一个事务，返回 tx 对象代表事务，执行、查询、事务提交、出错回滚。

把整个段落快做成一个函数，把分散的代码块变成大的代码块，这是代码重构中很常见的技巧。接下来只要 err 出错就终止函数执行，终止之前用 defer
判断错误不为空就执行回滚操作，这样只需要一个地方回滚操作。回滚操作不但要执行回滚操作，还要把错误保存起来，不能把两条语句写的到处都是非常不优雅。重构很常见技巧把分散的逻辑变成大的逻辑块，这样可以把分散逻辑重复的部分抽象出来变成统一处理，可以让代码变得很优雅。

一旦执行回滚操作以后，接下来提交操作不会执行的，如果回滚操作没有执行，提交操作可以正常执行。这样的好处是可以把事务处理过程变得很干净。

一个块启动事务、一个块提交事务、中间用一个匿名函数把整个事务中间过程打包。按照封装的原则打包，在内部不关心事务怎么启动，只关心要执行什么。中间部分还可以当成一个单独的函数。把这个过程抽象成一个框架，执行负责事务执行，下面负责处理具体的逻辑。把事务操作和逻辑操作分离。不要把执行策略和逻辑策略放在一起。

Begin() 启动事务，连接池里拿到连接用 begin 方式来启动，begin 同预处理方式类似通过底层 driver
处理，但是如果没有出错的情况下它会一直持有这个连接，并没有放弃这个连接，这和预处理是不一样的。

任何时候启动事务都不可能把这个事务时间放的很长，事务对于数据库来说它的资源消耗比预处理大得多，所以正常情况下总是事务开启后会立即执行执行完会立即提交。我们把这当做完整的操作，它属于同一个时间片内的事情。通常情况下会把这组合成一次调用，我们不会提前创建一个事务以后再反复使用。预处理编译完相当于在后端建立一个函数，这个函数是无状态的，但是事务必须有个结束操作，要么是提交要么是回滚，这就决定了开始结束中间必须是连续的。事务有很严重的问题在于它在数据库一端会启动锁机制，按行锁，事务的生命周期必须要足够短。

Exec 事务执行时候用 grabConn 方式获取连接，直接从事务属性中拿。然后通过 driver 执行相应的操作。

很显然事务对于连接管理和普通预处理管理方式是不同的，原因在于预处理的生命周期和事务是不一样的，这是两种不同的设计思想。

Commit 提交和 Rollback 回滚操作会检查 done 标记，如果提交过了不会重复操作，close
告诉服务器端关闭事务，把连接放到连接池里面去。我们启动事务数据库端也会准备相应的资源，它会把所有的接收数据放入临时表里面，然后一次性向数据库提交，所以事务在数据库端消耗相应的资源。所以出错时必须执行回滚操作，不执行回滚操作的话，这个连接会一直持有，直到
tx 对象被垃圾回收。

另外还有一个方式考虑类似表的方式来执行。把所有的执行语句用一个表统一管理起来，用同样一段代码执行表里面的语句。把执行代码和数据分离。这属于代码重构的事。

### Scan & Null

数据库有个很特殊的空值 NULL 转换成某个具体语言就比较麻烦，比如整数没有空值只有默认值 0，也就是对于不同的类型空值映射过来会很麻烦，用什么值代替？

所以标准库必须有种方法解决这个问题，怎么样处理空值，可能用字节码代替，还有种方式专门提供 sql.NullInt64
类型，内部提供相应的标记来判断是否为空值。

另外提供两个转换接口 Valuer 和 Scanner 说白了有个自动转换的过程。比如有个值写到数据库里去，假如传过去的数据是个需要加密的数据，明文 abc
最好有种手段完成加密保存数据库时候是乱码，乱码返回时候再读取之前完成解密操作还原成 abc，就需要特殊处理自定义一个类型实现接口，Value()
函数是写入时候用什么方式改变这个值，Scan() 函数是读取时候用什么方式读取这个值。

包装成一个特殊的类型，这个类型内部有两个方法完成写入和读取，编码过程和解码过程。可以用它做加密，也可以做默认值的处理，可以用来做参数检查。

### 第三方库推荐

> <https://github.com/gocraft/dbr>

在官方标准库基础之上改良的第三方库，它改良了官方设计的一些弊端，它能提高很高的速度，效率比官方版本高很多，使用方式像 LINQ
的方式。建议不必须用它代替官方标准库。只有在使用标准库性能出现问题的时候才会尝试使用第三方的，任何事一个系统当中首先要解决最慢的一个，如果标准库操作不是最慢的先不要管，优先使用标准库，标准库的升级更新质量相对来说有保障，它虽然不是最快的但是是最稳定的。

自动构建 SQL 语句：

  * <https://github.com/azer/crud>
  * <https://github.com/go-ozzo/ozzo-dbx>
  * <https://github.com/jmoiron/sqlx>
  * <https://github.com/doug-martin/goqu>
  * <https://github.com/Masterminds/squirrel>

> **感谢各位的光临哟！！**
> **获取更多资源:掘金小册,gitchat专栏,极客时间等资源；**
> **请到闲鱼店：583128058yanghon**
> **啊呜呜~~~**